#!/usr/bin/env python3
"""
Ollama é™çº§å¤„ç†æ¼”ç¤º

æ¼”ç¤ºå½“ Ollama æœ¬åœ°æœåŠ¡ä¸å¯ç”¨æ—¶çš„é™çº§å¤„ç†æœºåˆ¶ï¼ŒåŒ…æ‹¬ï¼š
- è‡ªåŠ¨æ£€æµ‹ Ollama å¯ç”¨æ€§
- å›é€€åˆ° OpenAI æœåŠ¡
- ä¼˜é›…çš„é”™è¯¯å¤„ç†
- ç”¨æˆ·å‹å¥½çš„æç¤ºä¿¡æ¯
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.llm.ollama_integration import (
    check_ollama_availability, get_ollama_embedding, generate_ollama_text,
    list_ollama_models, get_ollama_installation_guide, OllamaNotAvailableError
)
from src.llm.providers import get_api_key_from_keyring, ProviderType


async def demo_ollama_availability_check():
    """æ¼”ç¤º Ollama å¯ç”¨æ€§æ£€æŸ¥"""
    print("=== Ollama å¯ç”¨æ€§æ£€æŸ¥ ===")
    
    is_available = check_ollama_availability()
    
    if is_available:
        print("âœ… Ollama æœåŠ¡å¯ç”¨")
        print("å¯ä»¥ä½¿ç”¨æœ¬åœ° OSS æ¨¡å‹è¿›è¡Œ AI ä»»åŠ¡")
    else:
        print("âŒ Ollama æœåŠ¡ä¸å¯ç”¨")
        print("å°†ä½¿ç”¨äº‘ç«¯ AI æœåŠ¡ä½œä¸ºæ›¿ä»£")
    
    return is_available


async def demo_ollama_embedding_with_fallback():
    """æ¼”ç¤ºå¸¦é™çº§çš„åµŒå…¥åŠŸèƒ½"""
    print("\n=== åµŒå…¥åŠŸèƒ½æ¼”ç¤ºï¼ˆå¸¦é™çº§ï¼‰ ===")
    
    try:
        print("å°è¯•è·å–æ–‡æœ¬åµŒå…¥...")
        embedding = await get_ollama_embedding(
            text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ï¼Œç”¨äºæ¼”ç¤ºåµŒå…¥åŠŸèƒ½",
            model="llama2",
            fallback_to_openai=True
        )
        
        print(f"âœ… æˆåŠŸè·å–åµŒå…¥å‘é‡ï¼Œç»´åº¦: {len(embedding)}")
        print(f"å‰5ä¸ªå€¼: {embedding[:5]}")
        
    except OllamaNotAvailableError as e:
        print(f"âŒ Ollama ä¸å¯ç”¨: {e}")
        print("å»ºè®®:")
        print("1. å®‰è£…å¹¶å¯åŠ¨ Ollama æœåŠ¡")
        print("2. æˆ–è€…é…ç½® OpenAI API å¯†é’¥ä½œä¸ºå›é€€")
        
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")


async def demo_ollama_text_generation_with_fallback():
    """æ¼”ç¤ºå¸¦é™çº§çš„æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    print("\n=== æ–‡æœ¬ç”ŸæˆåŠŸèƒ½æ¼”ç¤ºï¼ˆå¸¦é™çº§ï¼‰ ===")
    
    try:
        print("å°è¯•ç”Ÿæˆæ–‡æœ¬...")
        text = await generate_ollama_text(
            prompt="è¯·ç”¨ä¸­æ–‡ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
            model="llama2",
            system="ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯ä¸“å®¶",
            fallback_to_openai=True
        )
        
        print(f"âœ… æˆåŠŸç”Ÿæˆæ–‡æœ¬:")
        print(f"å†…å®¹: {text[:200]}...")
        
    except OllamaNotAvailableError as e:
        print(f"âŒ Ollama ä¸å¯ç”¨: {e}")
        print("å»ºè®®:")
        print("1. å®‰è£…å¹¶å¯åŠ¨ Ollama æœåŠ¡")
        print("2. æˆ–è€…é…ç½® OpenAI API å¯†é’¥ä½œä¸ºå›é€€")
        
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")


async def demo_ollama_model_management_with_fallback():
    """æ¼”ç¤ºæ¨¡å‹ç®¡ç†åŠŸèƒ½ï¼ˆæ— é™çº§ï¼‰"""
    print("\n=== æ¨¡å‹ç®¡ç†åŠŸèƒ½æ¼”ç¤º ===")
    
    try:
        print("å°è¯•è·å–æ¨¡å‹åˆ—è¡¨...")
        models = await list_ollama_models()
        
        print(f"âœ… æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹:")
        for model in models:
            print(f"  - {model.name} ({model.size / (1024*1024*1024):.1f} GB)")
        
    except OllamaNotAvailableError as e:
        print(f"âŒ Ollama ä¸å¯ç”¨: {e}")
        print("æ¨¡å‹ç®¡ç†åŠŸèƒ½éœ€è¦æœ¬åœ° Ollama æœåŠ¡")
        print("è¯·å…ˆå®‰è£…å¹¶å¯åŠ¨ Ollama")
        
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")


async def demo_api_key_check():
    """æ¼”ç¤º API å¯†é’¥æ£€æŸ¥"""
    print("\n=== API å¯†é’¥æ£€æŸ¥ ===")
    
    # æ£€æŸ¥ OpenAI API å¯†é’¥
    openai_key = get_api_key_from_keyring(ProviderType.OPENAI)
    if openai_key:
        print("âœ… OpenAI API å¯†é’¥å·²é…ç½®")
        print("å¯ä»¥ä½œä¸º Ollama çš„å›é€€æœåŠ¡")
    else:
        print("âŒ OpenAI API å¯†é’¥æœªé…ç½®")
        print("æ— æ³•ä½¿ç”¨ OpenAI ä½œä¸ºå›é€€æœåŠ¡")
        print("å»ºè®®é…ç½® OpenAI API å¯†é’¥ä»¥æä¾›æ›´å¥½çš„é™çº§æ”¯æŒ")
    
    # æ£€æŸ¥ Claude API å¯†é’¥
    claude_key = get_api_key_from_keyring(ProviderType.CLAUDE)
    if claude_key:
        print("âœ… Claude API å¯†é’¥å·²é…ç½®")
    else:
        print("âŒ Claude API å¯†é’¥æœªé…ç½®")


async def demo_installation_guide():
    """æ¼”ç¤ºå®‰è£…æŒ‡å—"""
    print("\n=== Ollama å®‰è£…æŒ‡å— ===")
    
    guide = get_ollama_installation_guide()
    print(guide)


async def demo_error_handling_scenarios():
    """æ¼”ç¤ºå„ç§é”™è¯¯å¤„ç†åœºæ™¯"""
    print("\n=== é”™è¯¯å¤„ç†åœºæ™¯æ¼”ç¤º ===")
    
    scenarios = [
        {
            "name": "Ollama ä¸å¯ç”¨ï¼Œæ— å›é€€",
            "fallback": False,
            "description": "å¼ºåˆ¶ä½¿ç”¨ Ollamaï¼Œä¸å…è®¸å›é€€"
        },
        {
            "name": "Ollama ä¸å¯ç”¨ï¼Œæœ‰å›é€€",
            "fallback": True,
            "description": "å…è®¸å›é€€åˆ° OpenAI"
        }
    ]
    
    for scenario in scenarios:
        print(f"\nåœºæ™¯: {scenario['name']}")
        print(f"æè¿°: {scenario['description']}")
        
        try:
            embedding = await get_ollama_embedding(
                text="æµ‹è¯•æ–‡æœ¬",
                fallback_to_openai=scenario["fallback"]
            )
            print(f"âœ… æˆåŠŸ: è·å–åˆ° {len(embedding)} ç»´åµŒå…¥å‘é‡")
        except OllamaNotAvailableError as e:
            print(f"âŒ é¢„æœŸé”™è¯¯: {e}")
        except Exception as e:
            print(f"âŒ æ„å¤–é”™è¯¯: {e}")


async def demo_graceful_degradation():
    """æ¼”ç¤ºä¼˜é›…é™çº§"""
    print("\n=== ä¼˜é›…é™çº§æ¼”ç¤º ===")
    
    print("1. æ£€æŸ¥ Ollama å¯ç”¨æ€§...")
    ollama_available = check_ollama_availability()
    
    print("2. æ£€æŸ¥ API å¯†é’¥é…ç½®...")
    openai_available = get_api_key_from_keyring(ProviderType.OPENAI) is not None
    claude_available = get_api_key_from_keyring(ProviderType.CLAUDE) is not None
    
    print("3. ç¡®å®šå¯ç”¨çš„ AI æœåŠ¡...")
    available_services = []
    
    if ollama_available:
        available_services.append("Ollama (æœ¬åœ°)")
    if openai_available:
        available_services.append("OpenAI (äº‘ç«¯)")
    if claude_available:
        available_services.append("Claude (äº‘ç«¯)")
    
    if not available_services:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ AI æœåŠ¡")
        print("å»ºè®®:")
        print("1. å®‰è£… Ollama è¿›è¡Œæœ¬åœ° AI å¤„ç†")
        print("2. æˆ–è€…é…ç½® OpenAI/Claude API å¯†é’¥")
        return
    
    print(f"âœ… å¯ç”¨çš„ AI æœåŠ¡: {', '.join(available_services)}")
    
    # æ ¹æ®å¯ç”¨æœåŠ¡é€‰æ‹©æœ€ä½³ç­–ç•¥
    if ollama_available:
        print("ğŸ¯ æ¨èç­–ç•¥: ä½¿ç”¨æœ¬åœ° Ollama æœåŠ¡")
        print("ä¼˜åŠ¿: éšç§ä¿æŠ¤ã€æ— æˆæœ¬ã€ç¦»çº¿å¯ç”¨")
    elif openai_available:
        print("ğŸ¯ æ¨èç­–ç•¥: ä½¿ç”¨ OpenAI æœåŠ¡")
        print("ä¼˜åŠ¿: åŠŸèƒ½å®Œæ•´ã€æ€§èƒ½ç¨³å®š")
    elif claude_available:
        print("ğŸ¯ æ¨èç­–ç•¥: ä½¿ç”¨ Claude æœåŠ¡")
        print("ä¼˜åŠ¿: é•¿æ–‡æœ¬å¤„ç†ã€å›¾åƒåˆ†æ")
    
    print("\n4. æ‰§è¡Œ AI ä»»åŠ¡...")
    try:
        if ollama_available:
            print("ä½¿ç”¨ Ollama ç”Ÿæˆæ–‡æœ¬...")
            text = await generate_ollama_text(
                prompt="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                fallback_to_openai=False
            )
        elif openai_available:
            print("ä½¿ç”¨ OpenAI ç”Ÿæˆæ–‡æœ¬...")
            from src.llm.providers import create_provider, Message, ChatCompletionRequest
            provider = create_provider(ProviderType.OPENAI, api_key=get_api_key_from_keyring(ProviderType.OPENAI))
            request = ChatCompletionRequest(
                messages=[Message(role="user", content="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")],
                model="gpt-3.5-turbo"
            )
            response = await provider.chat_completion(request)
            text = response.choices[0]["message"]["content"]
        else:
            print("ä½¿ç”¨ Claude ç”Ÿæˆæ–‡æœ¬...")
            from src.llm.providers import create_provider, Message, ChatCompletionRequest
            provider = create_provider(ProviderType.CLAUDE, api_key=get_api_key_from_keyring(ProviderType.CLAUDE))
            request = ChatCompletionRequest(
                messages=[Message(role="user", content="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")],
                model="claude-3-haiku-20240307"
            )
            response = await provider.chat_completion(request)
            text = response.choices[0]["message"]["content"]
        
        print(f"âœ… æˆåŠŸç”Ÿæˆæ–‡æœ¬: {text[:100]}...")
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("Ollama é™çº§å¤„ç†æ¼”ç¤º\n")
    
    # åŸºç¡€æ£€æŸ¥
    await demo_ollama_availability_check()
    await demo_api_key_check()
    
    # åŠŸèƒ½æ¼”ç¤º
    await demo_ollama_embedding_with_fallback()
    await demo_ollama_text_generation_with_fallback()
    await demo_ollama_model_management_with_fallback()
    
    # é”™è¯¯å¤„ç†
    await demo_error_handling_scenarios()
    
    # ä¼˜é›…é™çº§
    await demo_graceful_degradation()
    
    # å®‰è£…æŒ‡å—
    await demo_installation_guide()
    
    print("\n=== æ¼”ç¤ºå®Œæˆ ===")
    print("\næ€»ç»“:")
    print("1. é¡¹ç›®ä¼šè‡ªåŠ¨æ£€æµ‹ Ollama å¯ç”¨æ€§")
    print("2. å½“ Ollama ä¸å¯ç”¨æ—¶ï¼Œä¼šå°è¯•å›é€€åˆ°äº‘ç«¯æœåŠ¡")
    print("3. æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œè§£å†³å»ºè®®")
    print("4. æ”¯æŒå¤šç§ AI æœåŠ¡çš„ä¼˜é›…é™çº§")
    print("5. ç¡®ä¿é¡¹ç›®åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ")


if __name__ == "__main__":
    asyncio.run(main()) 